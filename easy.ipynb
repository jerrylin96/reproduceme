{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v030.ib.bridges2.psc.edu\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 17:50:02.339282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 17:50:02.533243: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 17:50:03.904602: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/packages/cuda/v11.7.1/lib64:/opt/packages/cuda/v11.7.1/nvvm/lib64:/opt/packages/cuda/v11.7.1/extras/CUPTI/lib64:/jet/home/jlin96/.conda/envs/wandbenv/lib/\n",
      "2024-03-14 17:50:03.904731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/packages/cuda/v11.7.1/lib64:/opt/packages/cuda/v11.7.1/nvvm/lib64:/opt/packages/cuda/v11.7.1/extras/CUPTI/lib64:/jet/home/jlin96/.conda/envs/wandbenv/lib/\n",
      "2024-03-14 17:50:03.904739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "import tensorflow_addons as tfa\n",
    "from qhoptim.tf import QHAdamOptimizer\n",
    "from tensorflow.keras import callbacks\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/ocean/projects/atm200007p/jlin96/nnspreadtesting_2/overfitmepls/training/training_data/'\n",
    "train_input = np.load(data_path + 'train_input.npy')\n",
    "train_target = np.load(data_path + 'train_target.npy')\n",
    "val_input = np.load(data_path + 'val_input.npy')\n",
    "val_target = np.load(data_path + 'val_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjerrylin247365\u001b[0m (\u001b[33mcbrain\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 17:50:32.903753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 17:50:33.438584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14745 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjerrylin247365\u001b[0m (\u001b[33mcbrain\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/atm200007p/jlin96/nnspreadtesting_2/gpudebug/training/wandb/run-20240314_175035-5j5y2lbq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cbrain/debug221/runs/5j5y2lbq' target=\"_blank\">workffs</a></strong> to <a href='https://wandb.ai/cbrain/debug221' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cbrain/debug221' target=\"_blank\">https://wandb.ai/cbrain/debug221</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cbrain/debug221/runs/5j5y2lbq' target=\"_blank\">https://wandb.ai/cbrain/debug221/runs/5j5y2lbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cbrain/debug221/runs/5j5y2lbq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1507608cb0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(hp:dict):\n",
    "    alpha = hp[\"leak\"]\n",
    "    dp_rate = hp[\"dropout\"]\n",
    "    batch_norm = hp[\"batch_normalization\"]\n",
    "    model = Sequential()\n",
    "    hiddenUnits = hp['hidden_units']\n",
    "    model.add(Dense(units = hiddenUnits, input_dim=175, kernel_initializer='normal'))\n",
    "    model.add(LeakyReLU(alpha = alpha))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dp_rate))\n",
    "    for i in range(hp[\"num_layers\"]):\n",
    "        model.add(Dense(units = hiddenUnits, kernel_initializer='normal'))\n",
    "        model.add(LeakyReLU(alpha = alpha))\n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(dp_rate))\n",
    "    model.add(Dense(55, kernel_initializer='normal', activation='linear'))\n",
    "    initial_learning_rate = hp[\"lr\"]\n",
    "    optimizer = hp[\"optimizer\"]\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = initial_learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate = initial_learning_rate)\n",
    "    elif optimizer == \"RAdam\":\n",
    "        optimizer = tfa.optimizers.RectifiedAdam(learning_rate = initial_learning_rate)\n",
    "    elif optimizer == \"QHAdam\":\n",
    "        optimizer = QHAdamOptimizer(learning_rate = initial_learning_rate, nu2=1.0, beta1=0.995, beta2=0.999)\n",
    "    model.compile(optimizer = optimizer, loss = 'mse', metrics = [\"mse\"])\n",
    "    return model\n",
    "\n",
    "hp = {'leak':0.37343, \\\n",
    "      'dropout':0.042309, \\\n",
    "      'lr':0.0011942, \\\n",
    "      'hidden_units':336, \\\n",
    "      'num_layers':6, \\\n",
    "      'optimizer':\"RAdam\", \\\n",
    "      'batch_normalization':1}\n",
    "\n",
    "model = build_model(hp)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"debug221\", \n",
    "    # track hyperparameters and run metadata\n",
    "    config=hp,\n",
    "    name='workffs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "449/449 [==============================] - ETA: 0s - loss: 0.5710 - mse: 0.5710INFO:tensorflow:Assets written to: tuning_directory/debug221/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tuning_directory/debug221)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 19s 35ms/step - loss: 0.5710 - mse: 0.5710 - val_loss: 0.5925 - val_mse: 0.5925\n",
      "Epoch 2/5\n",
      "449/449 [==============================] - ETA: 0s - loss: 0.4200 - mse: 0.4200INFO:tensorflow:Assets written to: tuning_directory/debug221/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tuning_directory/debug221)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 13s 28ms/step - loss: 0.4200 - mse: 0.4200 - val_loss: 0.5715 - val_mse: 0.5715\n",
      "Epoch 3/5\n",
      "449/449 [==============================] - ETA: 0s - loss: 0.3980 - mse: 0.3980INFO:tensorflow:Assets written to: tuning_directory/debug221/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tuning_directory/debug221)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 12s 27ms/step - loss: 0.3980 - mse: 0.3980 - val_loss: 0.5621 - val_mse: 0.5621\n",
      "Epoch 4/5\n",
      "446/449 [============================>.] - ETA: 0s - loss: 0.3876 - mse: 0.3876INFO:tensorflow:Assets written to: tuning_directory/debug221/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tuning_directory/debug221)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 12s 27ms/step - loss: 0.3875 - mse: 0.3875 - val_loss: 0.5735 - val_mse: 0.5735\n",
      "Epoch 5/5\n",
      "448/449 [============================>.] - ETA: 0s - loss: 0.3809 - mse: 0.3809INFO:tensorflow:Assets written to: tuning_directory/debug221/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./tuning_directory/debug221)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 12s 28ms/step - loss: 0.3810 - mse: 0.3810 - val_loss: 0.5449 - val_mse: 0.5449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150760639400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_target, validation_data = (val_input, val_target), epochs = 5, batch_size = 5000, callbacks = [WandbMetricsLogger(), WandbModelCheckpoint((\"tuning_directory/debug221\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('wtf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
